{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import stats as sts\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import  argparse\n",
    "import torch\n",
    "import torchaudio\n",
    "from  torchaudio import  functional as F\n",
    "from nets.gtg import gen_gamma_3channel, gammatonegram_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "class image_loader(Dataset):\n",
    "    def __init__(self, data_dir, folds_file, test_fold, train_flag, params_json, input_transform=None, stetho_id=-1,\n",
    "                 aug_scale=None):\n",
    "\n",
    "        # getting device-wise information\n",
    "        self.file_to_device = {}\n",
    "        device_to_id = {}\n",
    "        device_id = 0\n",
    "        files = os.listdir(data_dir)\n",
    "        device_patient_list = []\n",
    "        pats = []\n",
    "        for f in files:\n",
    "            device = f.strip().split('_')[-1].split('.')[0]\n",
    "            if device not in device_to_id:\n",
    "                device_to_id[device] = device_id\n",
    "                device_id += 1\n",
    "                device_patient_list.append([])\n",
    "            self.file_to_device[f.strip().split('.')[0]] = device_to_id[device]\n",
    "            pat = f.strip().split('_')[0]\n",
    "            if pat not in device_patient_list[device_to_id[device]]:\n",
    "                device_patient_list[device_to_id[device]].append(pat)\n",
    "            if pat not in pats:\n",
    "                pats.append(pat)\n",
    "\n",
    "        print(\"DEVICE DICT\", device_to_id)\n",
    "        for idx in range(device_id):\n",
    "            print(\"Device\", idx, len(device_patient_list[idx]))\n",
    "\n",
    "        # get patients dict in current fold based on train flag\n",
    "        all_patients = open(folds_file).read().splitlines()  # 列表， 126个病人;  每个病人所对应的第几折中；\n",
    "        patient_dict = {}\n",
    "        for line in all_patients:  # 即当在当test_fold =4 时，  此时， patient_dict[] 只会有101 项病人， 不会包含第四折中的病人；\n",
    "            idx, fold = line.strip().split(' ')\n",
    "            if train_flag and int(fold) != test_fold:\n",
    "                patient_dict[idx] = fold\n",
    "            elif train_flag == False and int(fold) == test_fold:\n",
    "                patient_dict[idx] = fold\n",
    "\n",
    "        #extracting the audiofilenames and the data for breathing cycle and it's label\n",
    "        print(\n",
    "            \"Getting filenames ...\")  # filenames: 920 份文件名称， rec_annotation_dict: 920  个字典， 每个包含了该音频的分段标注信息， 即起始 ，终止时间， 标签类别；\n",
    "        filenames, rec_annotations_dict = get_annotations(data_dir)\n",
    "        if stetho_id >= 0:\n",
    "            self.filenames = [s for s in filenames if\n",
    "                              s.split('_')[0] in patient_dict and self.file_to_device[s] == stetho_id]\n",
    "        else:\n",
    "            self.filenames = [s for s in filenames if s.split('_')[0] in patient_dict]\n",
    "            # self.filenames = 722:   从filenames(920)分 中取出  patient_dict 的病人， 即只会取出 训练集中的编号病人，  即第4折中的病人数据 都没有包含，\n",
    "        self.audio_data = []  # each sample is a tuple with id_0: audio_data, id_1: label, id_2: file_name, id_3: cycle id, id_4: aug id, id_5: split id\n",
    "        self.labels = []\n",
    "        self.train_flag = train_flag\n",
    "        self.data_dir = data_dir\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "        # parameters for spectrograms\n",
    "        self.sample_rate = 4000\n",
    "        self.desired_length = 8\n",
    "        self.n_mels = 64\n",
    "        self.nfft = 256\n",
    "        self.hop = self.nfft // 2\n",
    "        self.f_max = 2000\n",
    "\n",
    "        self.dump_images = False\n",
    "        self.filenames_with_labels = []\n",
    "\n",
    "        # get individual breathing cycles from each audio file\n",
    "        print(\"Exracting Individual Cycles\")\n",
    "        self.cycle_list = []  # self.cycle_list: 从训练集中的总共4折的病人中， 生成5454份，呼吸音音频；\n",
    "        self.classwise_cycle_list = [[], [], [], []]\n",
    "\n",
    "        self.classes_with_duration_list = [[], [], [], []]\n",
    "        # 按照类别将，　　每个类别下各个子音频的持续时间添加到其中；\n",
    "\n",
    "        for idx, file_name in tqdm(enumerate(self.filenames)):\n",
    "            data = get_sound_samples(rec_annotations_dict[file_name], file_name, data_dir, self.sample_rate)\n",
    "            cycles_with_labels = [(d[0], d[3], file_name, cycle_idx, 0) for cycle_idx, d in enumerate(data[1:])]\n",
    "            self.cycle_list.extend(cycles_with_labels)\n",
    "            for cycle_idx, d in enumerate(cycles_with_labels):\n",
    "                self.filenames_with_labels.append(file_name + '_' + str(d[3]) + '_' + str(d[1]))\n",
    "                self.classwise_cycle_list[d[1]].append(d)\n",
    "\n",
    "            # 1. 统计出四个类别下， 每个类别下, 各自样本所持续的时间； dur =  end - start;\n",
    "            for cycle_in_curr_record, cur_data in enumerate(data[1:]):\n",
    "                cycle_dur = cur_data[2] - cur_data[1]\n",
    "                #　由于此时的 cur_data[3] 代表的是子音频的标签，　所以范围0-3　符合四个列表的范围；\n",
    "                self.classes_with_duration_list[cur_data[3]].append(cycle_dur)\n",
    "\n",
    "        if train_flag:\n",
    "            print(\" in the traindataset :\\n\")\n",
    "            print(\" the number of normal samples: \\n\", len(self.classes_with_duration_list[0]))\n",
    "\n",
    "            normal_list = np.array(self.classes_with_duration_list[0])\n",
    "            plt.hist(normal_list, bins = 25)\n",
    "\n",
    "            print(\" the number of crackle samples: \\n\", len(self.classes_with_duration_list[1]))\n",
    "            crackle_list = np.array(self.classes_with_duration_list[1])\n",
    "            plt.hist(crackle_list, bins = 25)\n",
    "\n",
    "            print(\" the number of wheeze samples: \\n\", len(self.classes_with_duration_list[2]))\n",
    "            wheeze_list = np.array(self.classes_with_duration_list[2])\n",
    "            plt.hist(wheeze_list, bins = 25)\n",
    "\n",
    "            print(\" the number of both  samples: \\n\", len(self.classes_with_duration_list[3]))\n",
    "            normal_list = np.array(self.classes_with_duration_list[3])\n",
    "            plt.hist(normal_list, bins = 25)\n",
    "\n",
    "\n",
    "        if not train_flag:\n",
    "            print(\" in the testdataset :\\n\")\n",
    "\n",
    "            normal = self.classes_with_duration_list[0]\n",
    "            print(\" the number of normal samples: \\n\", len(normal))\n",
    "            print('  {} < Fraction of samples < {} seconds:{} \\n'.format(0,2,\n",
    "                                                           np.sum(  normal>0 & normal < 2)/len(normal)))\n",
    "            normal_list = np.array(self.classes_with_duration_list[0])\n",
    "            plt.hist(normal_list, bins = 25)\n",
    "\n",
    "            print(\" the number of crackle samples: \\n\", len(self.classes_with_duration_list[1]))\n",
    "            crackle_list = np.array(self.classes_with_duration_list[1])\n",
    "            plt.hist(crackle_list, bins = 25)\n",
    "\n",
    "            print(\" the number of wheeze samples: \\n\", len(self.classes_with_duration_list[2]))\n",
    "            wheeze_list = np.array(self.classes_with_duration_list[2])\n",
    "            plt.hist(wheeze_list, bins = 25)\n",
    "\n",
    "            print(\" the number of both  samples: \\n\", len(self.classes_with_duration_list[3]))\n",
    "            normal_list = np.array(self.classes_with_duration_list[3])\n",
    "            plt.hist(normal_list, bins = 25)\n",
    "\n",
    "        # # concatenation based augmentation scheme\n",
    "        # if train_flag and aug_scale:\n",
    "        #     self.new_augment(scale=aug_scale)\n",
    "        #\n",
    "        # # split and pad each cycle to the desired length\n",
    "        # for idx, sample in enumerate(self.cycle_list):\n",
    "        #     output = split_and_pad(sample, self.desired_length, self.sample_rate, types=1)\n",
    "        #     self.audio_data.extend(output)\n",
    "        # # self.audio_data: 生成5471  份， 为什么 ！= 5454 份呼吸音；   从 # self.cycle_list 从训练集中的5454 份，呼吸音音频从\n",
    "        # self.device_wise = []\n",
    "        # for idx in range(device_id):\n",
    "        #     self.device_wise.append([])\n",
    "        # self.class_probs = np.zeros(4)\n",
    "        # self.identifiers = []\n",
    "        # for idx, sample in enumerate(self.audio_data):\n",
    "        #     self.class_probs[sample[1]] += 1.0\n",
    "        #     self.labels.append(sample[1])\n",
    "        #     self.identifiers.append(sample[2] + '_' + str(sample[3]) + '_' + str(sample[1]))\n",
    "        #     self.device_wise[self.file_to_device[sample[2]]].append(sample)\n",
    "        #\n",
    "        # if self.train_flag:\n",
    "        #     print(\"TRAIN DETAILS\")\n",
    "        # else:\n",
    "        #     print(\"TEST DETAILS\")\n",
    "        #\n",
    "        # print(\"CLASSWISE SAMPLE COUNTS:\", self.class_probs)\n",
    "        # print(\"Device to ID\", device_to_id)\n",
    "        # for idx in range(device_id):\n",
    "        #     print(\"DEVICE ID\", idx, \"size\", len(self.device_wise[idx]))\n",
    "        # self.class_probs = self.class_probs / sum(self.class_probs)\n",
    "        # print(\"CLASSWISE PROBS\", self.class_probs)\n",
    "        # print(\"LEN AUDIO DATA\", len(self.audio_data))\n",
    "\n",
    "    # def __getitem__(self, index):\n",
    "    #\n",
    "    #     audio = self.audio_data[index][0]\n",
    "    #\n",
    "    #     aug_prob = random.random()\n",
    "    #     if self.train_flag and aug_prob > 0.5:\n",
    "    #         # apply augmentation to audio\n",
    "    #         audio = gen_augmented(audio, self.sample_rate)\n",
    "    #\n",
    "    #         # pad incase smaller than desired length\n",
    "    #         audio = split_and_pad([audio, 0, 0, 0, 0], self.desired_length, self.sample_rate, types=1)[0][0]\n",
    "    #\n",
    "    #     # roll audio sample\n",
    "    #     roll_prob = random.random()\n",
    "    #     if self.train_flag and roll_prob > 0.5:\n",
    "    #         audio = rollAudio(audio)\n",
    "    #\n",
    "    #     # convert audio signal to spectrogram\n",
    "    #     # spectrograms resized to 3x of original size\n",
    "    #     audio_image = cv2.cvtColor(create_mel_raw(audio, self.sample_rate, f_max=self.f_max,\n",
    "    #                                               n_mels=self.n_mels, nfft=self.nfft, hop=self.hop, resz=3),\n",
    "    #                                cv2.COLOR_BGR2RGB)\n",
    "    #\n",
    "    #     # blank region clipping\n",
    "    #     audio_raw_gray = cv2.cvtColor(create_mel_raw(audio, self.sample_rate, f_max=self.f_max,\n",
    "    #                                                  n_mels=self.n_mels, nfft=self.nfft, hop=self.hop),\n",
    "    #                                   cv2.COLOR_BGR2GRAY)\n",
    "    #     # print(\"\\n  audio raw_gray  shape \", audio_raw_gray.shape )\n",
    "    #     # ;torch.Size([3, 192, 753])   audio raw_gray  shape  (64, 251)\n",
    "    #\n",
    "    #     audio_raw_gray[audio_raw_gray < 10] = 0\n",
    "    #     for row in range(audio_raw_gray.shape[0]):\n",
    "    #         black_percent = len(np.where(audio_raw_gray[row, :] == 0)[0]) / len(audio_raw_gray[row, :])\n",
    "    #         if black_percent < 0.80:\n",
    "    #             break\n",
    "    #\n",
    "    #     # 　如果此时的行数减少了，　行数变成 row + 1;\n",
    "    #     if (row + 1) * 3 < audio_image.shape[0]:\n",
    "    #         # 　此时将剩余的行，代表了频域信息，　乘以３倍， 扩张后的mel 语谱图仍然< n_mels*3 时，则使用线性插值的方式，统一语谱图的大小；\n",
    "    #         audio_image = audio_image[(row + 1) * 3:, :, :]\n",
    "    #     audio_image = cv2.resize(audio_image, (audio_image.shape[1], self.n_mels * 3), interpolation=cv2.INTER_LINEAR)\n",
    "    #\n",
    "    #     if self.dump_images:\n",
    "    #         save_images((audio_image, self.audio_data[index][2], self.audio_data[index][3],\n",
    "    #                      self.audio_data[index][5], self.audio_data[index][1]), self.train_flag)\n",
    "    #\n",
    "    #     # label\n",
    "    #     label = self.audio_data[index][1]\n",
    "    #\n",
    "    #     # apply image transform\n",
    "    #     if self.input_transform is not None:\n",
    "    #         audio_image = self.input_transform(audio_image)\n",
    "    #\n",
    "    #     # print(\"\\n  audio image  shape \", audio_image.shape )\n",
    "    #     # ;torch.Size([3, 192, 753])\n",
    "    #\n",
    "    #     return audio_image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# --data_dir ./data/ICBHI_final_database/ --folds_file ./data/patient_list_foldwise.txt --model_path models_out --lr 1e-3 --batch_size 1 --num_worker 8 --start_epochs 0 --epochs 200 --test_fold 4 --checkpoint ./models/ckpt_best.pkl\n",
    "\n",
    "data_dir = './data/ICBHI_final_database/'\n",
    "folds_file = './data/patient_list_foldwise.txt'\n",
    "test_fold = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE DICT {'AKGC417L': 0, 'LittC2SE': 1, 'Meditron': 2, 'Litt3200': 3}\n",
      "Device 0 32\n",
      "Device 1 23\n",
      "Device 2 64\n",
      "Device 3 11\n",
      "Getting filenames ...\n",
      "Exracting Individual Cycles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [02:36,  1.15s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "audio_image = image_loader(data_dir, folds_file,test_fold, False, \"Params_json\", input_transform=None, stetho_id=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}